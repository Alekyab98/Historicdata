import csv
import json
import subprocess
from datetime import datetime, timezone
from urllib.parse import quote

# Function to load API keys from a file
def load_api_keys(filename):
    api_keys = {}
    with open(filename, 'r') as file:
        for line in file:
            line = line.strip()
            if line:
                function_name, key = line.split(':', 1)
                api_keys[function_name] = key.strip()
    return api_keys


def convert_epoch_to_timestamp(epoch_time):
    return datetime.fromtimestamp(int(epoch_time), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')


def execute_curl_and_get_data(curl_command):
    result = subprocess.run(curl_command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Error executing curl command: {result.stderr}")
        return None
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON response: {e}")
        return None


def process_metrics(input_files, api_keys, output_file):
    start_epoch = '1731801600'  
    end_epoch = '1731884400'
    step = '1h'

    data_rows = []

    for input_file, function_name in input_files.items():
        encoded_api_key = api_keys.get(function_name)
        if not encoded_api_key:
            print(f"Skipping {function_name} due to missing API key.")
            continue

        with open(input_file, 'r') as file:
            for metric_name in file:
                metric_name = metric_name.strip()
                if not metric_name:
                    continue

                for query_type, column_name in [("sum_over_time", "metric_sum"), ("increase", "metric_increase")]:
                    query = f'{query_type}({metric_name}[{step}])'
                    encoded_query = quote(query)
                    curl_command = (
                        f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                        f"query={encoded_query}&start={start_epoch}&end={end_epoch}&step={step}\""
                        f" --header \"Authorization: Basic {encoded_api_key}\""
                    )

                    print(f"Generated curl command for {metric_name} ({query_type}):\n{curl_command}\n")

                    response_data = execute_curl_and_get_data(curl_command)
                    if not response_data or response_data.get('status') != 'success':
                        print(f"No data found for {metric_name} ({query_type}).")
                        continue

                    for result in response_data['data']['result']:
                        fqdn = (
                            result['metric'].get('kubernetes_namespace', 'unknown')
                            if function_name in ['amf', 'smf']
                            else result['metric'].get('localdn', 'unknown')
                        )
                        labels = {
                            k: v
                            for k, v in result['metric'].items()
                            if k not in ['__name__', 'kubernetes_namespace', 'localdn']
                        }
                        for value in result['values']:
                            event_time = convert_epoch_to_timestamp(value[0])
                            metric_value = value[1]
                            data_rows.append({
                                "metric_name": metric_name,
                                "FQDN": fqdn,
                                "event_time": event_time,
                                column_name: metric_value,
                                "labels": json.dumps(labels),
                            })

    
    with open(output_file, mode='w', newline='') as csv_file:
        fieldnames = ["metric_name", "FQDN", "event_time", "metric_sum", "metric_increase", "labels"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()

       
        merged_data = {}
        for row in data_rows:
            key = (row["metric_name"], row["FQDN"], row["event_time"])
            if key not in merged_data:
                merged_data[key] = row
            else:
                merged_data[key].update(row)

        for row in merged_data.values():
            writer.writerow(row)

    print(f"CSV file created: {output_file}")


if __name__ == "__main__":
    # Input files
    input_files = {
        "amf_metrics.txt": "amf",
        "smf_metrics.txt": "smf",
        "upf_metrics.txt": "upf",
    }

    # API keys file
    api_keys_file = "api_keys.txt"

    # Output CSV file
    output_file = "metrics_results.csv"

    
    api_keys = load_api_keys(api_keys_file)

    process_metrics(input_files, api_keys, output_file)
