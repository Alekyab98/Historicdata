import csv
import json
import subprocess
from datetime import datetime, timezone
from urllib.parse import quote

def load_api_keys(filename):
    api_keys = {}
    with open(filename, 'r') as file:
        for line in file:
            if line.strip():
                function_name, key = line.strip().split(':', 1)
                api_keys[function_name] = key.strip()
    return api_keys

def convert_epoch_to_timestamp(epoch_time):
    return datetime.fromtimestamp(int(epoch_time), tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

def extract_date_from_timestamp(timestamp):
    return timestamp.split(' ')[0]

def safe_json_dumps(data):
    try:
        return json.dumps(data)
    except Exception as e:
        print("JSON Error:", e, data)
        return "{}"

def execute_curl(curl_command):
    result = subprocess.run(curl_command, shell=True, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"Curl command failed: {result.stderr}")
        return None
    try:
        return json.loads(result.stdout)
    except json.JSONDecodeError:
        print("Failed to parse JSON from response.")
        return None

def fetch_raw_metric_data(curl_command, metric_name):
    response = execute_curl(curl_command)
    if not response or response.get('status') != 'success':
        print(f"No data returned for {metric_name}")
        return []

    rows = []
    for result in response['data']['result']:
        labels = result.get('metric', {})
        fqdn = labels.get('kubernetes_namespace') or labels.get('localdn') or 'unknown'
        for value in result.get('values', []):
            timestamp = convert_epoch_to_timestamp(value[0])
            transdt = extract_date_from_timestamp(timestamp)
            raw_value = value[1]
            rows.append({
                "metric_name": metric_name,
                "FQDN": fqdn,
                "labels": safe_json_dumps(labels),
                "event_time": timestamp,
                "transdt": transdt,
                "raw_value": raw_value
            })
    return rows

def process_raw_metrics(input_file, api_key, output_file):
    start_epoch = '1740096000'
    end_epoch = '1740783600'
    step = '1m'

    with open(input_file, 'r') as f:
        metrics = [line.strip() for line in f if line.strip()]

    with open(output_file, 'w', newline='') as csvfile:
        fieldnames = ["metric_name", "FQDN", "labels", "event_time", "transdt", "raw_value"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for metric_name in metrics:
            query = f'{metric_name} offset -1h'
            curl_cmd = (
                f"curl --location --globoff \"https://us.aether.nss.vzwnet.com/gem/prometheus/api/v1/query_range?"
                f"query={quote(query)}&start={start_epoch}&end={end_epoch}&step={step}\" "
                f"--header \"Authorization: Basic {api_key}\""
            )
            rows = fetch_raw_metric_data(curl_cmd, metric_name)
            for row in rows:
                writer.writerow(row)

    print(f"Raw metric data written to {output_file}")

# === MAIN ===
if __name__ == "__main__":
    metric_file = "smf_metrics.txt"
    api_keys = load_api_keys("api_keys.txt")
    api_key = api_keys.get("smf")
    output_csv = "smf_raw_data_0221_0228.csv"

    if api_key:
        process_raw_metrics(metric_file, api_key, output_csv)
    else:
        print("SMF API key not found. Please check api_keys.txt")
